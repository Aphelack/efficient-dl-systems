{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fec75e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:29:37.663258Z",
     "iopub.status.busy": "2026-02-09T01:29:37.662803Z",
     "iopub.status.idle": "2026-02-09T01:30:06.528157Z",
     "shell.execute_reply": "2026-02-09T01:30:06.527424Z",
     "shell.execute_reply.started": "2026-02-09T01:29:37.663232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import Sampler, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6553c56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:30:06.529405Z",
     "iopub.status.busy": "2026-02-09T01:30:06.529001Z",
     "iopub.status.idle": "2026-02-09T01:30:07.124392Z",
     "shell.execute_reply": "2026-02-09T01:30:07.123599Z",
     "shell.execute_reply.started": "2026-02-09T01:30:06.529383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ad7d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:30:09.216032Z",
     "iopub.status.busy": "2026-02-09T01:30:09.215649Z",
     "iopub.status.idle": "2026-02-09T01:30:09.233094Z",
     "shell.execute_reply": "2026-02-09T01:30:09.232224Z",
     "shell.execute_reply.started": "2026-02-09T01:30:09.216012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "def get_gpt2_model(device = 'cpu') -> torch.nn.Module:\n",
    "    return TransformerModel(\n",
    "        ntoken=tokenizer.vocab_size,\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        d_hid=1024,\n",
    "        nlayers=8,\n",
    "        dropout=0.1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904d3155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:30:12.176896Z",
     "iopub.status.busy": "2026-02-09T01:30:12.176470Z",
     "iopub.status.idle": "2026-02-09T01:30:24.048582Z",
     "shell.execute_reply": "2026-02-09T01:30:24.047846Z",
     "shell.execute_reply.started": "2026-02-09T01:30:12.176876Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_wikitext_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read().split('\\n = ')\n",
    "\n",
    "def get_train_samples(data_path):\n",
    "    articles_1 =  process_wikitext_file(f\"{data_path}/train-00000-of-00002.txt\")\n",
    "    articles_2 =  process_wikitext_file(f\"{data_path}/train-00001-of-00002.txt\")\n",
    "    articles = articles_1 + articles_2\n",
    "    return articles\n",
    "\n",
    "data_path = 'wikitext-103-raw-v1'\n",
    "articles = get_train_samples(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb226ce-3614-4f8d-b88a-8c637b59926d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:30:24.049963Z",
     "iopub.status.busy": "2026-02-09T01:30:24.049639Z",
     "iopub.status.idle": "2026-02-09T01:30:55.288722Z",
     "shell.execute_reply": "2026-02-09T01:30:55.287740Z",
     "shell.execute_reply.started": "2026-02-09T01:30:24.049942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenizer(\n",
    "    articles,\n",
    "    return_tensors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771ae6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:30:55.289955Z",
     "iopub.status.busy": "2026-02-09T01:30:55.289624Z",
     "iopub.status.idle": "2026-02-09T01:30:55.303020Z",
     "shell.execute_reply": "2026-02-09T01:30:55.302266Z",
     "shell.execute_reply.started": "2026-02-09T01:30:55.289935Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 640\n",
    "\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples: str, max_length: int = MAX_LENGTH):\n",
    "        tokens = tokenized_samples['input_ids']\n",
    "        max_len_in_batch = max(len(seq) for seq in tokens)\n",
    "\n",
    "        padded_sequences = []\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_length:\n",
    "                seq = seq[:max_length]\n",
    "            if len(seq) < max_length:\n",
    "                seq = seq + [0] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        self.samples = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        assert idx >= 0 and idx < len(self.samples)\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed0a884-890e-49b6-b67b-2d91e3eefe06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:31:45.671075Z",
     "iopub.status.busy": "2026-02-09T01:31:45.670641Z",
     "iopub.status.idle": "2026-02-09T01:31:45.687951Z",
     "shell.execute_reply": "2026-02-09T01:31:45.687233Z",
     "shell.execute_reply.started": "2026-02-09T01:31:45.671054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = MAX_LENGTH):\n",
    "        self.max_length = max_length\n",
    "        num_samples = len(tokenized_samples['input_ids'])\n",
    "\n",
    "        self.samples_tensor = torch.full((num_samples, max_length), \n",
    "                                        0, dtype=torch.long)\n",
    "        \n",
    "        self.lengths = torch.zeros(num_samples, dtype=torch.long)\n",
    "\n",
    "        for i, seq in enumerate(tokenized_samples['input_ids']):\n",
    "            length = min(len(seq), max_length)\n",
    "            self.lengths[i] = length\n",
    "            self.samples_tensor[i, :length] = torch.tensor(seq[:length], dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.samples_tensor[idx], self.lengths[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_tensor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensors, lengths = zip(*batch)\n",
    "    stacked = torch.stack(tensors)\n",
    "    max_len_in_batch = max(lengths)\n",
    "\n",
    "    return stacked[:, :max_len_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163f6c09-9793-46d9-9c78-94f8d2b316c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:31:46.558155Z",
     "iopub.status.busy": "2026-02-09T01:31:46.557613Z",
     "iopub.status.idle": "2026-02-09T01:31:46.618867Z",
     "shell.execute_reply": "2026-02-09T01:31:46.618125Z",
     "shell.execute_reply.started": "2026-02-09T01:31:46.558135Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from typing import List\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "MAX_LENGTH = 640\n",
    "\n",
    "def pad_and_truncate_tokens(token_lists, max_length: int = MAX_LENGTH):\n",
    "    padded = []\n",
    "    for tokens in token_lists:\n",
    "        truncated = tokens[:max_length]\n",
    "        if len(truncated) < max_length:\n",
    "            truncated += [0] * (max_length - len(truncated))\n",
    "        padded.append(truncated)\n",
    "    return padded\n",
    "\n",
    "class UltraBigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = MAX_LENGTH, n_bins: int = 10):\n",
    "        self.max_length = max_length\n",
    "        input_ids = tokenized_samples['input_ids']\n",
    "        self.real_lengths = [min(max_length, len(seq)) for seq in input_ids]\n",
    "        self.samples = pad_and_truncate_tokens(input_ids, max_length)\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = self._create_bins()\n",
    "        for bin_id in self.bins:\n",
    "            random.shuffle(self.bins[bin_id])\n",
    "    \n",
    "    def _create_bins(self):\n",
    "        bins = defaultdict(list)\n",
    "        if not self.real_lengths:\n",
    "            return bins\n",
    "        min_len = min(self.real_lengths)\n",
    "        max_len = max(self.real_lengths)\n",
    "        if min_len == max_len or self.n_bins == 1:\n",
    "            bins[0] = list(range(len(self.samples)))\n",
    "            return bins\n",
    "        bin_size = (max_len - min_len) / self.n_bins\n",
    "        for idx, length in enumerate(self.real_lengths):\n",
    "            bin_id = min(self.n_bins - 1, int((length - min_len) // bin_size))\n",
    "            bins[bin_id].append(idx)\n",
    "        return {k: v for k, v in bins.items() if v}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx], dtype=torch.long), self.real_lengths[idx]\n",
    "\n",
    "class UltraBigBrainBatchSampler(Sampler):\n",
    "    def __init__(self, dataset: UltraBigBrainDataset, batch_size: int, k: int = 10):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        \n",
    "        self.length_to_indices = defaultdict(list)\n",
    "        for idx, length in enumerate(dataset.real_lengths):\n",
    "            self.length_to_indices[length].append(idx)\n",
    "        \n",
    "        self.sorted_lengths = sorted(self.length_to_indices.keys())\n",
    "        for length in self.sorted_lengths:\n",
    "            random.shuffle(self.length_to_indices[length])\n",
    "        \n",
    "        self.batches = self._create_batches()\n",
    "    \n",
    "    def _create_batches(self):\n",
    "        batches = []\n",
    "        length_to_indices = {k: v.copy() for k, v in self.length_to_indices.items()}\n",
    "        sorted_lengths = self.sorted_lengths.copy()\n",
    "        \n",
    "        while sorted_lengths:\n",
    "            batch = []\n",
    "            start_length = sorted_lengths[0]\n",
    "            available_lengths = [\n",
    "                length for length in sorted_lengths \n",
    "                if abs(length - start_length) <= self.k\n",
    "            ]\n",
    "            \n",
    "            while len(batch) < self.batch_size and available_lengths:\n",
    "                current_length = available_lengths[0]\n",
    "                if not length_to_indices[current_length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if current_length in sorted_lengths:\n",
    "                        sorted_lengths.remove(current_length)\n",
    "                    continue\n",
    "                \n",
    "                batch.append(length_to_indices[current_length].pop())\n",
    "                if len(batch) == self.batch_size:\n",
    "                    break\n",
    "                \n",
    "                if not length_to_indices[current_length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if current_length in sorted_lengths:\n",
    "                        sorted_lengths.remove(current_length)\n",
    "            \n",
    "            if batch:\n",
    "                batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = self.batches.copy()\n",
    "        random.shuffle(batches)\n",
    "        yield from batches\n",
    "\n",
    "def ultra_brain_collate_fn(batch_items):\n",
    "    tensors, lengths = zip(*batch_items)\n",
    "    max_len = max(lengths)\n",
    "    trimmed_tensors = [tensor[:max_len] for tensor in tensors]\n",
    "    return torch.stack(trimmed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beb471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T03:44:27.044404Z",
     "iopub.status.busy": "2026-02-09T03:44:27.043428Z",
     "iopub.status.idle": "2026-02-09T03:44:27.075068Z",
     "shell.execute_reply": "2026-02-09T03:44:27.073968Z",
     "shell.execute_reply.started": "2026-02-09T03:44:27.044381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import bisect\n",
    "\n",
    "class UltraDuperBigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, packing_type, max_length=MAX_LENGTH):\n",
    "        packed_samples = []\n",
    "        packed_seq_ids = []\n",
    "\n",
    "        samples = [s[:max_length] for s in tokenized_samples[\"input_ids\"]]\n",
    "\n",
    "        if packing_type == \"basic\":\n",
    "            packed_samples, packed_seq_ids = self._basic_pack(samples, max_length)\n",
    "        elif packing_type == \"ffd\":\n",
    "            packed_samples, packed_seq_ids = self._ffd_pack(samples, max_length)\n",
    "        elif packing_type == \"obfd\":\n",
    "            packed_samples, packed_seq_ids = self._obfd_pack(samples, max_length)\n",
    "        else:\n",
    "            raise ValueError(packing_type)\n",
    "\n",
    "        self.input_ids = torch.cat(packed_samples, dim=0)\n",
    "        self.seq_ids = torch.cat(packed_seq_ids, dim=0)\n",
    "\n",
    "\n",
    "    def _basic_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "        buf, buf_ids = [], []\n",
    "        sid = 0\n",
    "\n",
    "        for s in samples:\n",
    "            for t in s:\n",
    "                buf.append(t)\n",
    "                buf_ids.append(sid)\n",
    "                if len(buf) == max_length:\n",
    "                    inputs.append(self._make_input(buf, max_length))\n",
    "                    seqs.append(self._make_seq_ids(buf_ids, max_length))\n",
    "                    buf, buf_ids = [], []\n",
    "            sid += 1\n",
    "\n",
    "        if buf:\n",
    "            inputs.append(self._make_input(buf, max_length))\n",
    "            seqs.append(self._make_seq_ids(buf_ids, max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    def _ffd_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "\n",
    "        sequences = [(i, s) for i, s in enumerate(samples) if len(s) <= max_length]\n",
    "        sequences.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        bins = []\n",
    "        free = []\n",
    "\n",
    "        for sid, seq in sequences:\n",
    "            l = len(seq)\n",
    "            idx = bisect.bisect_left(free, (l, -1))\n",
    "            if idx < len(free):\n",
    "                _, bidx = free.pop(idx)\n",
    "                b = bins[bidx]\n",
    "                b[\"tok\"].extend(seq)\n",
    "                b[\"ids\"].extend([sid] * l)\n",
    "                b[\"rem\"] -= l\n",
    "                bisect.insort(free, (b[\"rem\"], bidx))\n",
    "            else:\n",
    "                bidx = len(bins)\n",
    "                bins.append({\"tok\": list(seq), \"ids\": [sid] * l, \"rem\": max_length - l})\n",
    "                bisect.insort(free, (max_length - l, bidx))\n",
    "        for b in bins:\n",
    "            inputs.append(self._make_input(b[\"tok\"], max_length))\n",
    "            seqs.append(self._make_seq_ids(b[\"ids\"], max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    def _obfd_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "\n",
    "        sequences = [(i, s) for i, s in enumerate(samples) if len(s) <= max_length]\n",
    "        sequences.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        bins = []\n",
    "        free = []\n",
    "\n",
    "        for sid, seq in sequences:\n",
    "            l = len(seq)\n",
    "            idx = bisect.bisect_left(free, (l, -1))\n",
    "\n",
    "            if idx < len(free):\n",
    "                _, bidx = free.pop(idx)\n",
    "                b = bins[bidx]\n",
    "                b[\"tok\"].extend(seq)\n",
    "                b[\"ids\"].extend([sid] * l)\n",
    "                b[\"rem\"] -= l\n",
    "                bisect.insort(free, (b[\"rem\"], bidx))\n",
    "            else:\n",
    "                bidx = len(bins)\n",
    "                bins.append({\n",
    "                    \"tok\": list(seq),\n",
    "                    \"ids\": [sid] * l,\n",
    "                    \"rem\": max_length - l,\n",
    "                })\n",
    "                bisect.insort(free, (max_length - l, bidx))\n",
    "\n",
    "        for b in bins:\n",
    "            inputs.append(self._make_input(b[\"tok\"], max_length))\n",
    "            seqs.append(self._make_seq_ids(b[\"ids\"], max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    def _make_input(self, tokens, max_length):\n",
    "        return torch.tensor(\n",
    "            tokens + [0] * (max_length - len(tokens)),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def _make_seq_ids(self, seq_ids, max_length):\n",
    "        return torch.tensor(\n",
    "            seq_ids + [-1] * (max_length - len(seq_ids)),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"seq_ids\": self.seq_ids[idx],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62116bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:31:50.141514Z",
     "iopub.status.busy": "2026-02-09T01:31:50.141095Z",
     "iopub.status.idle": "2026-02-09T01:31:50.697356Z",
     "shell.execute_reply": "2026-02-09T01:31:50.696569Z",
     "shell.execute_reply.started": "2026-02-09T01:31:50.141493Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.benchmark import Timer\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "class DataMode(Enum):\n",
    "    BRAIN = 1\n",
    "    BIG_BRAIN = 2\n",
    "    ULTRA_BIG_BRAIN = 3\n",
    "    ULTRA_DUPER_BIG_BRAIN = 4\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "def get_dataloader(data_mode: DataMode, n_bins: int | None = None, k: int | None = None, packing_type: str | None = None):\n",
    "    if data_mode == DataMode.BRAIN:\n",
    "        dataset = BrainDataset(tokenized_samples)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32, pin_memory=True)\n",
    "    if data_mode == DataMode.BIG_BRAIN:\n",
    "        dataset = BigBrainDataset(tokenized_samples)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=32, pin_memory=True)\n",
    "    if data_mode == DataMode.ULTRA_BIG_BRAIN:\n",
    "        if n_bins is None:\n",
    "            n_bins = 5\n",
    "        if k is None:\n",
    "            k = 5\n",
    "        dataset = UltraBigBrainDataset(tokenized_samples, n_bins=n_bins)\n",
    "        sampler = UltraBigBrainBatchSampler(\n",
    "            dataset=dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            k=k,\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_sampler=sampler, \n",
    "            num_workers=32,\n",
    "            pin_memory=True,\n",
    "            collate_fn=ultra_brain_collate_fn\n",
    "        )\n",
    "    if data_mode == DataMode.ULTRA_DUPER_BIG_BRAIN:\n",
    "        dataset = UltraDuperBigBrainDataset(tokenized_samples, packing_type)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7ef6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:31:52.855850Z",
     "iopub.status.busy": "2026-02-09T01:31:52.854998Z",
     "iopub.status.idle": "2026-02-09T01:31:52.872325Z",
     "shell.execute_reply": "2026-02-09T01:31:52.871667Z",
     "shell.execute_reply.started": "2026-02-09T01:31:52.855819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142453e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:19:44.985464Z",
     "iopub.status.busy": "2026-02-02T00:19:44.984155Z",
     "iopub.status.idle": "2026-02-02T00:19:45.049819Z",
     "shell.execute_reply": "2026-02-02T00:19:45.049112Z",
     "shell.execute_reply.started": "2026-02-02T00:19:44.985437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def run_epoch(data_mode: DataMode, model, optimizer, criterion, device, \n",
    "              warmup_batches=3, n_bins: int | None = None, k: int | None = None):\n",
    "    dataloader = get_dataloader(data_mode, n_bins, k)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(device.type) if device.type == 'cuda' else None\n",
    "    batch_times = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_batch_size = 0\n",
    "    total_seq_len = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        is_warmup = batch_idx < warmup_batches\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        if not is_warmup:\n",
    "            start_time = time.perf_counter()\n",
    "        \n",
    "        inputs = batch.to(device)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len = inputs.size(1)\n",
    "        total_batch_size += batch_size\n",
    "        total_seq_len += batch_size * seq_len\n",
    "        \n",
    "        targets = inputs[:, 1:]\n",
    "        inputs = inputs[:, :-1]\n",
    "        src = inputs.transpose(0, 1)\n",
    "        tgt_y = targets.reshape(-1)\n",
    "        mask = generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                out = model(src, mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                logits = out.reshape(-1, out.size(-1))\n",
    "                loss = criterion(logits, tgt_y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        if not is_warmup:\n",
    "            batch_time = time.perf_counter() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if is_warmup:\n",
    "            progress_bar.set_postfix(\n",
    "                status=\"warmup\", \n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len\n",
    "            )\n",
    "        else:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
    "            \n",
    "            avg_batch_size = total_batch_size / (batch_idx + 1)\n",
    "            avg_seq_len = total_seq_len / total_batch_size\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                avg_loss=f\"{avg_loss:.4f}\",\n",
    "                batch_time=f\"{batch_time:.4f}s\",\n",
    "                avg_batch_time=f\"{avg_batch_time:.4f}s\",\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "                avg_batch_size=f\"{avg_batch_size:.1f}\",\n",
    "                avg_seq_len=f\"{avg_seq_len:.1f}\"\n",
    "            )\n",
    "\n",
    "    time_stats = {}\n",
    "    if batch_times:\n",
    "        time_array = np.array(batch_times)\n",
    "        time_stats = {\n",
    "            'batch_time_min': float(np.min(time_array)),\n",
    "            'batch_time_max': float(np.max(time_array)),\n",
    "            'batch_time_mean': float(np.mean(time_array)),\n",
    "            'batch_time_median': float(np.median(time_array)),\n",
    "            'batch_time_std': float(np.std(time_array)),\n",
    "            'num_batches': len(batch_times)\n",
    "        }\n",
    "        \n",
    "        total_time = sum(batch_times)\n",
    "        throughput = total_samples / total_time\n",
    "        avg_time = total_time / len(batch_times)\n",
    "        final_avg_loss = total_loss / total_samples\n",
    "        \n",
    "        final_avg_batch_size = total_batch_size / len(progress_bar)\n",
    "        final_avg_seq_len = total_seq_len / total_batch_size\n",
    "        \n",
    "        print(f\"Avg loss: {final_avg_loss:.4f}\")\n",
    "        print(f\"Avg batch time: {avg_time:.4f}s\")\n",
    "        print(f\"Throughput: {throughput:.2f} samples/s\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Avg batch size: {final_avg_batch_size:.2f}\")\n",
    "        print(f\"Avg sequence length: {final_avg_seq_len:.2f}\")\n",
    "        print(f\"Batch time stats: min={time_stats['batch_time_min']:.4f}s, \"\n",
    "              f\"max={time_stats['batch_time_max']:.4f}s, \"\n",
    "              f\"mean={time_stats['batch_time_mean']:.4f}s, \"\n",
    "              f\"median={time_stats['batch_time_median']:.4f}s\")\n",
    "    \n",
    "    stats = {\n",
    "        'avg_loss': final_avg_loss if batch_times else 0,\n",
    "        'throughput': throughput if batch_times else 0,\n",
    "        'total_samples': total_samples,\n",
    "        'avg_batch_size': final_avg_batch_size if batch_times else 0,\n",
    "        'avg_seq_len': final_avg_seq_len if batch_times else 0,\n",
    "        'time_stats': time_stats,\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d376b1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:00:31.670496Z",
     "iopub.status.busy": "2026-02-02T01:00:31.670002Z",
     "iopub.status.idle": "2026-02-02T01:15:59.838015Z",
     "shell.execute_reply": "2026-02-02T01:15:59.837008Z",
     "shell.execute_reply.started": "2026-02-02T01:00:31.670465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [15:07<00:00,  5.26it/s, avg_batch_size=64.0, avg_batch_time=0.1861s, avg_loss=2.7787, avg_seq_len=640.0, batch_size=26, batch_time=0.0858s, loss=2.5784, seq_len=640]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 2.7787\n",
      "Avg batch time: 0.1861s\n",
      "Throughput: 344.13 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 640.00\n",
      "Batch time stats: min=0.0858s, max=0.2001s, mean=0.1861s, median=0.1861s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_brain = run_epoch(\n",
    "    DataMode.BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518d147e-5f37-4ebf-9dbb-2ae712184128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:34:26.161770Z",
     "iopub.status.busy": "2026-02-09T01:34:26.161325Z",
     "iopub.status.idle": "2026-02-09T01:34:26.176241Z",
     "shell.execute_reply": "2026-02-09T01:34:26.175479Z",
     "shell.execute_reply.started": "2026-02-09T01:34:26.161746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"This DataLoader will create\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The current process just got forked\")\n",
    "\n",
    "# Fix tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccdcd62f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:23:11.266473Z",
     "iopub.status.busy": "2026-02-02T01:23:11.265496Z",
     "iopub.status.idle": "2026-02-02T01:38:31.183847Z",
     "shell.execute_reply": "2026-02-02T01:38:31.182880Z",
     "shell.execute_reply.started": "2026-02-02T01:23:11.266446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [15:07<00:00,  5.26it/s, avg_batch_size=64.0, avg_batch_time=0.1860s, avg_loss=2.7496, avg_seq_len=640.0, batch_size=26, batch_time=0.0850s, loss=2.3751, seq_len=640]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 2.7496\n",
      "Avg batch time: 0.1860s\n",
      "Throughput: 344.25 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 640.00\n",
      "Batch time stats: min=0.0850s, max=0.1935s, mean=0.1860s, median=0.1860s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_big_brain = run_epoch(\n",
    "    DataMode.BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4058286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:18:20.594274Z",
     "iopub.status.busy": "2026-02-01T23:18:20.593155Z",
     "iopub.status.idle": "2026-02-01T23:18:20.686082Z",
     "shell.execute_reply": "2026-02-01T23:18:20.685143Z",
     "shell.execute_reply.started": "2026-02-01T23:18:20.594244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16741844463793543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([1 if len(t) > 640 else 0 for t in tokenized_samples['input_ids']]) / len(tokenized_samples['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9056d92-4545-48cb-a679-b7e90d8e2362",
   "metadata": {},
   "source": [
    "For batch 64 at most always there are samples with seq_len > 640 in batch, so we dont see avg_batch_time improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0c661ff-1f62-4b53-9fbe-8825093a69af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:20:21.978428Z",
     "iopub.status.busy": "2026-02-01T23:20:21.977439Z",
     "iopub.status.idle": "2026-02-01T23:20:22.009891Z",
     "shell.execute_reply": "2026-02-01T23:20:22.008974Z",
     "shell.execute_reply.started": "2026-02-01T23:20:21.978386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('brain.json', 'w') as f:\n",
    "    json.dump(batch_times_brain, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dc7603c-8c9c-49c3-995d-7753b8d82857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:20:37.937832Z",
     "iopub.status.busy": "2026-02-01T23:20:37.937376Z",
     "iopub.status.idle": "2026-02-01T23:20:37.964674Z",
     "shell.execute_reply": "2026-02-01T23:20:37.963827Z",
     "shell.execute_reply.started": "2026-02-01T23:20:37.937809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('big_brain.json', 'w') as f:\n",
    "    json.dump(batch_times_big_brain, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d9bb5dd-7ac2-4c5e-a76d-2558c9c29524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:19:48.777658Z",
     "iopub.status.busy": "2026-02-02T00:19:48.777156Z",
     "iopub.status.idle": "2026-02-02T00:28:18.504484Z",
     "shell.execute_reply": "2026-02-02T00:28:18.503318Z",
     "shell.execute_reply.started": "2026-02-02T00:19:48.777630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:17<00:00,  9.59it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.2030, avg_seq_len=324.5, batch_size=64, batch_time=0.0767s, loss=4.9446, seq_len=246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.2030\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.29 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0214s, max=0.2078s, mean=0.1011s, median=0.0947s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k10 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d3ed3df-93df-44da-9d04-964e4cd97fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:29:53.599554Z",
     "iopub.status.busy": "2026-02-02T00:29:53.598858Z",
     "iopub.status.idle": "2026-02-02T00:38:20.907023Z",
     "shell.execute_reply": "2026-02-02T00:38:20.905999Z",
     "shell.execute_reply.started": "2026-02-02T00:29:53.599511Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:18<00:00,  9.58it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.2091, avg_seq_len=324.5, batch_size=64, batch_time=0.1216s, loss=5.0844, seq_len=406] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.2091\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.22 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0219s, max=0.2013s, mean=0.1011s, median=0.0945s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k1 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f97f9b7-580d-44d9-baf0-3291b693d2cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:38:39.811658Z",
     "iopub.status.busy": "2026-02-02T00:38:39.811148Z",
     "iopub.status.idle": "2026-02-02T00:47:04.030004Z",
     "shell.execute_reply": "2026-02-02T00:47:04.028880Z",
     "shell.execute_reply.started": "2026-02-02T00:38:39.811632Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:18<00:00,  9.57it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.1857, avg_seq_len=324.5, batch_size=64, batch_time=0.1042s, loss=4.9141, seq_len=347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.1857\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.10 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0219s, max=0.2019s, mean=0.1011s, median=0.0948s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k50 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb937d-b25a-46fe-b94a-0cc8c5d5c6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T02:34:37.375430Z",
     "iopub.status.busy": "2026-02-09T02:34:37.374480Z",
     "iopub.status.idle": "2026-02-09T02:34:37.403329Z",
     "shell.execute_reply": "2026-02-09T02:34:37.402649Z",
     "shell.execute_reply.started": "2026-02-09T02:34:37.375408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_epoch_ultra_duper_big_brain(\n",
    "    data_mode: DataMode,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    warmup_batches=3,\n",
    "    n_bins: int | None = None,\n",
    "    k: int | None = None,\n",
    "    packing_type: str | None = None,\n",
    "):\n",
    "    dataloader = get_dataloader(data_mode, n_bins, k, packing_type)\n",
    "    model.train().to(device)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device.type) if device.type == \"cuda\" else None\n",
    "\n",
    "    batch_times = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_batch_size = 0\n",
    "    total_seq_len = 0\n",
    "\n",
    "    nhead = model.transformer_encoder.layers[0].self_attn.num_heads\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        is_warmup = batch_idx < warmup_batches\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        if not is_warmup:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        seq_ids   = batch[\"seq_ids\"].to(device)\n",
    "\n",
    "        inputs  = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "        seq_ids = seq_ids[:, :-1]\n",
    "\n",
    "        B, L = inputs.shape\n",
    "\n",
    "        causal = torch.tril(\n",
    "            torch.ones(L, L, device=device, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        same_seq = seq_ids.unsqueeze(1) == seq_ids.unsqueeze(2)\n",
    "\n",
    "        allowed = causal & same_seq\n",
    "        allowed.diagonal(dim1=-2, dim2=-1).fill_(True)\n",
    "\n",
    "        attn_mask = ~allowed\n",
    "\n",
    "        attn_mask = (\n",
    "            attn_mask.unsqueeze(1)\n",
    "            .expand(-1, nhead, -1, -1)\n",
    "            .reshape(B * nhead, L, L)\n",
    "        )\n",
    "\n",
    "        total_batch_size += B\n",
    "        total_seq_len += B * L\n",
    "\n",
    "        src = inputs.transpose(0, 1)\n",
    "        tgt_y = targets.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler:\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                out = model(src, attn_mask)\n",
    "                logits = out.transpose(0, 1).reshape(-1, out.size(-1))\n",
    "                loss = criterion(logits, tgt_y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(src, attn_mask)\n",
    "            logits = out.transpose(0, 1).reshape(-1, out.size(-1))\n",
    "            loss = criterion(logits, tgt_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        if not is_warmup:\n",
    "            batch_time = time.perf_counter() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * B\n",
    "        total_samples += B\n",
    "\n",
    "        if is_warmup:\n",
    "            progress_bar.set_postfix(\n",
    "                status=\"warmup\",\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                batch_size=B,\n",
    "                seq_len=L,\n",
    "            )\n",
    "        else:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "            avg_batch_size = total_batch_size / (batch_idx + 1)\n",
    "            avg_seq_len = total_seq_len / total_batch_size\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                avg_loss=f\"{avg_loss:.4f}\",\n",
    "                batch_time=f\"{batch_time:.4f}s\",\n",
    "                avg_batch_time=f\"{avg_batch_time:.4f}s\",\n",
    "                batch_size=B,\n",
    "                seq_len=L,\n",
    "                avg_batch_size=f\"{avg_batch_size:.1f}\",\n",
    "                avg_seq_len=f\"{avg_seq_len:.1f}\",\n",
    "            )\n",
    "\n",
    "    time_stats = {}\n",
    "    if batch_times:\n",
    "        time_array = np.array(batch_times)\n",
    "        time_stats = {\n",
    "            'batch_time_min': float(np.min(time_array)),\n",
    "            'batch_time_max': float(np.max(time_array)),\n",
    "            'batch_time_mean': float(np.mean(time_array)),\n",
    "            'batch_time_median': float(np.median(time_array)),\n",
    "            'batch_time_std': float(np.std(time_array)),\n",
    "            'num_batches': len(batch_times)\n",
    "        }\n",
    "        \n",
    "        total_time = sum(batch_times)\n",
    "        throughput = total_samples / total_time\n",
    "        avg_time = total_time / len(batch_times)\n",
    "        final_avg_loss = total_loss / total_samples\n",
    "        \n",
    "        final_avg_batch_size = total_batch_size / len(progress_bar)\n",
    "        final_avg_seq_len = total_seq_len / total_batch_size\n",
    "        \n",
    "        print(f\"Avg loss: {final_avg_loss:.4f}\")\n",
    "        print(f\"Avg batch time: {avg_time:.4f}s\")\n",
    "        print(f\"Throughput: {throughput:.2f} samples/s\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Avg batch size: {final_avg_batch_size:.2f}\")\n",
    "        print(f\"Avg sequence length: {final_avg_seq_len:.2f}\")\n",
    "        print(f\"Batch time stats: min={time_stats['batch_time_min']:.4f}s, \"\n",
    "              f\"max={time_stats['batch_time_max']:.4f}s, \"\n",
    "              f\"mean={time_stats['batch_time_mean']:.4f}s, \"\n",
    "              f\"median={time_stats['batch_time_median']:.4f}s\")\n",
    "    \n",
    "    stats = {\n",
    "        'avg_loss': final_avg_loss if batch_times else 0,\n",
    "        'throughput': throughput if batch_times else 0,\n",
    "        'total_samples': total_samples,\n",
    "        'avg_batch_size': final_avg_batch_size if batch_times else 0,\n",
    "        'avg_seq_len': final_avg_seq_len if batch_times else 0,\n",
    "        'time_stats': time_stats,\n",
    "    }\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81c811bd-c4ad-4ee1-9c2a-51c4dbae53a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T03:18:24.944524Z",
     "iopub.status.busy": "2026-02-09T03:18:24.944057Z",
     "iopub.status.idle": "2026-02-09T03:35:56.839439Z",
     "shell.execute_reply": "2026-02-09T03:35:56.838730Z",
     "shell.execute_reply.started": "2026-02-09T03:18:24.944502Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2420/2420 [16:57<00:00,  2.38it/s, avg_batch_size=64.0, avg_batch_time=0.4152s, avg_loss=5.6307, avg_seq_len=639.0, batch_size=64, batch_time=0.4112s, loss=4.9201, seq_len=639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.6307\n",
      "Avg batch time: 0.4152s\n",
      "Throughput: 154.34 samples/s\n",
      "Total samples: 154880\n",
      "Avg batch size: 64.00\n",
      "Avg sequence length: 639.00\n",
      "Batch time stats: min=0.4087s, max=4.3061s, mean=0.4152s, median=0.4127s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "batch_times_ultra_duper_big_brain_basic = run_epoch_ultra_duper_big_brain(\n",
    "    DataMode.ULTRA_DUPER_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    packing_type='basic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8377c287-3a19-4955-a17d-0a71bdaec4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T03:44:35.162834Z",
     "iopub.status.busy": "2026-02-09T03:44:35.162439Z",
     "iopub.status.idle": "2026-02-09T04:02:13.994441Z",
     "shell.execute_reply": "2026-02-09T04:02:13.993492Z",
     "shell.execute_reply.started": "2026-02-09T03:44:35.162813Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2421/2421 [16:59<00:00,  2.38it/s, avg_batch_size=64.0, avg_batch_time=0.4154s, avg_loss=5.6092, avg_seq_len=639.0, batch_size=8, batch_time=0.1762s, loss=4.7823, seq_len=639] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.6092\n",
      "Avg batch time: 0.4154s\n",
      "Throughput: 154.19 samples/s\n",
      "Total samples: 154888\n",
      "Avg batch size: 63.98\n",
      "Avg sequence length: 639.00\n",
      "Batch time stats: min=0.1762s, max=3.8509s, mean=0.4154s, median=0.4130s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "batch_times_ultra_duper_big_brain_ffd = run_epoch_ultra_duper_big_brain(\n",
    "    DataMode.ULTRA_DUPER_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    packing_type='ffd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1a30028-7a23-4156-9729-2f33f48dca6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T02:56:53.882383Z",
     "iopub.status.busy": "2026-02-09T02:56:53.881936Z",
     "iopub.status.idle": "2026-02-09T03:14:28.080004Z",
     "shell.execute_reply": "2026-02-09T03:14:28.079184Z",
     "shell.execute_reply.started": "2026-02-09T02:56:53.882361Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2421/2421 [16:56<00:00,  2.38it/s, avg_batch_size=64.0, avg_batch_time=0.4144s, avg_loss=5.6000, avg_seq_len=639.0, batch_size=8, batch_time=0.2089s, loss=5.0453, seq_len=639] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.6000\n",
      "Avg batch time: 0.4144s\n",
      "Throughput: 154.56 samples/s\n",
      "Total samples: 154888\n",
      "Avg batch size: 63.98\n",
      "Avg sequence length: 639.00\n",
      "Batch time stats: min=0.2089s, max=4.2795s, mean=0.4144s, median=0.4118s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "batch_times_ultra_duper_big_brain_obfd = run_epoch_ultra_duper_big_brain(\n",
    "    DataMode.ULTRA_DUPER_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    packing_type='obfd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9e714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>config</th>\n",
       "      <th>avg_loss</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>avg_batch_size</th>\n",
       "      <th>avg_seq_len</th>\n",
       "      <th>batch_time_min</th>\n",
       "      <th>batch_time_max</th>\n",
       "      <th>batch_time_mean</th>\n",
       "      <th>batch_time_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ultrabigbrain k=1</td>\n",
       "      <td>5.209138</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.992040</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.201310</td>\n",
       "      <td>0.101121</td>\n",
       "      <td>0.094524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ultrabigbrain k=10</td>\n",
       "      <td>5.203040</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.992040</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.207838</td>\n",
       "      <td>0.101111</td>\n",
       "      <td>0.094678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ultrabigbrain k=50</td>\n",
       "      <td>5.185742</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.992040</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.201899</td>\n",
       "      <td>0.101141</td>\n",
       "      <td>0.094805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>big brain</td>\n",
       "      <td>2.749565</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.992040</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.085038</td>\n",
       "      <td>0.193472</td>\n",
       "      <td>0.186007</td>\n",
       "      <td>0.186009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>brain</td>\n",
       "      <td>2.778737</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.992040</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.085790</td>\n",
       "      <td>0.200119</td>\n",
       "      <td>0.186070</td>\n",
       "      <td>0.186064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>ultraduperbigbrain obfd</td>\n",
       "      <td>5.600036</td>\n",
       "      <td>154888</td>\n",
       "      <td>63.976869</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>0.208876</td>\n",
       "      <td>4.279453</td>\n",
       "      <td>0.414430</td>\n",
       "      <td>0.411835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>ultraduperbigbrain basic</td>\n",
       "      <td>5.630688</td>\n",
       "      <td>154880</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>0.408687</td>\n",
       "      <td>4.306136</td>\n",
       "      <td>0.415173</td>\n",
       "      <td>0.412682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>ultraduperbigbrain ffd</td>\n",
       "      <td>5.609242</td>\n",
       "      <td>154888</td>\n",
       "      <td>63.976869</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>0.176223</td>\n",
       "      <td>3.850917</td>\n",
       "      <td>0.415424</td>\n",
       "      <td>0.412960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                    config  avg_loss  total_samples  avg_batch_size  \\\n",
       "0      0         ultrabigbrain k=1  5.209138         305498       63.992040   \n",
       "1      1        ultrabigbrain k=10  5.203040         305498       63.992040   \n",
       "2      2        ultrabigbrain k=50  5.185742         305498       63.992040   \n",
       "3      3                 big brain  2.749565         305498       63.992040   \n",
       "4      4                     brain  2.778737         305498       63.992040   \n",
       "5      6   ultraduperbigbrain obfd  5.600036         154888       63.976869   \n",
       "6      7  ultraduperbigbrain basic  5.630688         154880       64.000000   \n",
       "7      8    ultraduperbigbrain ffd  5.609242         154888       63.976869   \n",
       "\n",
       "   avg_seq_len  batch_time_min  batch_time_max  batch_time_mean  \\\n",
       "0   324.526956        0.021912        0.201310         0.101121   \n",
       "1   324.526956        0.021415        0.207838         0.101111   \n",
       "2   324.526956        0.021850        0.201899         0.101141   \n",
       "3   640.000000        0.085038        0.193472         0.186007   \n",
       "4   640.000000        0.085790        0.200119         0.186070   \n",
       "5   639.000000        0.208876        4.279453         0.414430   \n",
       "6   639.000000        0.408687        4.306136         0.415173   \n",
       "7   639.000000        0.176223        3.850917         0.415424   \n",
       "\n",
       "   batch_time_median  \n",
       "0           0.094524  \n",
       "1           0.094678  \n",
       "2           0.094805  \n",
       "3           0.186009  \n",
       "4           0.186064  \n",
       "5           0.411835  \n",
       "6           0.412682  \n",
       "7           0.412960  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('report (1).json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dfe09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623d4ae4-668d-4865-b62f-28308e8291b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:18:56.118255Z",
     "iopub.status.busy": "2026-02-01T21:18:56.117689Z",
     "iopub.status.idle": "2026-02-01T21:27:30.525492Z",
     "shell.execute_reply": "2026-02-01T21:27:30.524481Z",
     "shell.execute_reply.started": "2026-02-01T21:18:56.118232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting datasets==2.9.0\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting einops==0.7.0\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gdown==4.7.3\n",
      "  Downloading gdown-4.7.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting matplotlib==3.8.2\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Collecting py-spy==0.3.14\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
      "Collecting transformers==4.48.2\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Collecting torchvision==0.19.0\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting tqdm==4.64.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "Collecting vit_pytorch==0.40.2\n",
      "  Downloading vit_pytorch-0.40.2-py3-none-any.whl.metadata (702 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.12.2)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (9.0.0)\n",
      "Collecting dill<0.3.7 (from datasets==2.9.0)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (2.27.1)\n",
      "Collecting xxhash (from datasets==2.9.0)\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.9.0)\n",
      "  Downloading multiprocess-0.70.19-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (3.8.5)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0 (from datasets==2.9.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (23.1)\n",
      "Collecting responses<0.19 (from datasets==2.9.0)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0) (6.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown==4.7.3) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.7.3) (4.11.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2022.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2022.10.31)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.2)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.48.2) (0.7.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (1.2.0)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /usr/local/lib/python3.10/dist-packages (from responses<0.19->datasets==2.9.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.9.0) (3.4)\n",
      "INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.7.3) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.9.0)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.7.3) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m  \u001b[33m0:00:18\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Downloading gdown-4.7.3-py3-none-any.whl (16 kB)\n",
      "Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Downloading vit_pytorch-0.40.2-py3-none-any.whl (83 kB)\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m  \u001b[33m0:00:25\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: py-spy, xxhash, typing-extensions, triton, tqdm, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, dill, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, matplotlib, huggingface-hub, tokenizers, nvidia-cusolver-cu12, gdown, transformers, torch, datasets, torchvision, torchtext, vit_pytorch\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/31\u001b[0m [matplotlib]s]cu12]12]2]2]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.3.5━━━━━━━━\u001b[0m \u001b[32m20/31\u001b[0m [matplotlib]\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.3.5:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m21/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.3.50m━━━━━━━━━━━━\u001b[0m \u001b[32m21/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: tokenizers━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m21/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.2━━━━━━━━━━━━\u001b[0m \u001b[32m21/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.2:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m21/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.20m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m22/31\u001b[0m [tokenizers]]\n",
      "\u001b[2K  Attempting uninstall: transformers━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m24/31\u001b[0m [gdown]-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: transformers 5.0.00m━━━━━━━━━\u001b[0m \u001b[32m24/31\u001b[0m [gdown]\n",
      "\u001b[2K    Uninstalling transformers-5.0.0:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m25/31\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-5.0.0╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m25/31\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31/31\u001b[0m [vit_pytorch]\u001b[0m [vit_pytorch]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.4.0 which is incompatible.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.4.0 which is incompatible.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.9.0 dill-0.3.6 einops-0.7.0 gdown-4.7.3 huggingface-hub-0.36.0 matplotlib-3.8.2 multiprocess-0.70.14 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 py-spy-0.3.14 responses-0.18.0 tokenizers-0.21.4 torch-2.4.0 torchtext-0.18.0 torchvision-0.19.0 tqdm-4.64.1 transformers-4.48.2 triton-3.0.0 typing-extensions-4.15.0 vit_pytorch-0.40.2 xxhash-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.4.0 datasets==2.9.0 einops==0.7.0 gdown==4.7.3 matplotlib==3.8.2 pandas==1.5.3 py-spy==0.3.14 transformers==4.48.2 torchtext torchvision==0.19.0 tqdm==4.64.1 vit_pytorch==0.40.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fec75e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:53:13.240687Z",
     "iopub.status.busy": "2026-02-01T21:53:13.240160Z",
     "iopub.status.idle": "2026-02-01T21:53:37.858265Z",
     "shell.execute_reply": "2026-02-01T21:53:37.857347Z",
     "shell.execute_reply.started": "2026-02-01T21:53:13.240663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import Sampler, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6553c56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:53:37.860001Z",
     "iopub.status.busy": "2026-02-01T21:53:37.859550Z",
     "iopub.status.idle": "2026-02-01T21:53:38.434734Z",
     "shell.execute_reply": "2026-02-01T21:53:38.433750Z",
     "shell.execute_reply.started": "2026-02-01T21:53:37.859979Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ad7d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:53:38.436043Z",
     "iopub.status.busy": "2026-02-01T21:53:38.435677Z",
     "iopub.status.idle": "2026-02-01T21:53:38.453976Z",
     "shell.execute_reply": "2026-02-01T21:53:38.453112Z",
     "shell.execute_reply.started": "2026-02-01T21:53:38.436021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "def get_gpt2_model(device = 'cpu') -> torch.nn.Module:\n",
    "    return TransformerModel(\n",
    "        ntoken=tokenizer.vocab_size,\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        d_hid=1024,\n",
    "        nlayers=8,\n",
    "        dropout=0.1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904d3155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:53:38.455604Z",
     "iopub.status.busy": "2026-02-01T21:53:38.455262Z",
     "iopub.status.idle": "2026-02-01T21:53:50.610340Z",
     "shell.execute_reply": "2026-02-01T21:53:50.609335Z",
     "shell.execute_reply.started": "2026-02-01T21:53:38.455584Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_wikitext_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read().split('\\n = ')\n",
    "\n",
    "def get_train_samples(data_path):\n",
    "    articles_1 =  process_wikitext_file(f\"{data_path}/train-00000-of-00002.txt\")\n",
    "    articles_2 =  process_wikitext_file(f\"{data_path}/train-00001-of-00002.txt\")\n",
    "    articles = articles_1 + articles_2\n",
    "    return articles\n",
    "\n",
    "data_path = 'wikitext-103-raw-v1'\n",
    "articles = get_train_samples(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb226ce-3614-4f8d-b88a-8c637b59926d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:53:50.612052Z",
     "iopub.status.busy": "2026-02-01T21:53:50.611711Z",
     "iopub.status.idle": "2026-02-01T21:54:22.307049Z",
     "shell.execute_reply": "2026-02-01T21:54:22.306093Z",
     "shell.execute_reply.started": "2026-02-01T21:53:50.612030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenizer(\n",
    "    articles,\n",
    "    return_tensors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "771ae6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:00:27.520088Z",
     "iopub.status.busy": "2026-02-02T01:00:27.519547Z",
     "iopub.status.idle": "2026-02-02T01:00:27.540225Z",
     "shell.execute_reply": "2026-02-02T01:00:27.539557Z",
     "shell.execute_reply.started": "2026-02-02T01:00:27.520058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 640\n",
    "\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples: str, max_length: int = MAX_LENGTH):\n",
    "        tokens = tokenized_samples['input_ids']\n",
    "        max_len_in_batch = max(len(seq) for seq in tokens)\n",
    "\n",
    "        padded_sequences = []\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_length:\n",
    "                seq = seq[:max_length]\n",
    "            if len(seq) < max_length:\n",
    "                seq = seq + [0] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        self.samples = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        assert idx >= 0 and idx < len(self.samples)\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ed0a884-890e-49b6-b67b-2d91e3eefe06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:59:02.182437Z",
     "iopub.status.busy": "2026-02-02T00:59:02.181975Z",
     "iopub.status.idle": "2026-02-02T00:59:02.198767Z",
     "shell.execute_reply": "2026-02-02T00:59:02.197733Z",
     "shell.execute_reply.started": "2026-02-02T00:59:02.182414Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = MAX_LENGTH):\n",
    "        self.max_length = max_length\n",
    "        num_samples = len(tokenized_samples['input_ids'])\n",
    "\n",
    "        self.samples_tensor = torch.full((num_samples, max_length), \n",
    "                                        0, dtype=torch.long)\n",
    "        \n",
    "        self.lengths = torch.zeros(num_samples, dtype=torch.long)\n",
    "\n",
    "        for i, seq in enumerate(tokenized_samples['input_ids']):\n",
    "            length = min(len(seq), max_length)\n",
    "            self.lengths[i] = length\n",
    "            self.samples_tensor[i, :length] = torch.tensor(seq[:length], dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.samples_tensor[idx], self.lengths[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_tensor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensors, lengths = zip(*batch)\n",
    "    stacked = torch.stack(tensors)\n",
    "    max_len_in_batch = max(lengths)\n",
    "\n",
    "    return stacked[:, :max_len_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "163f6c09-9793-46d9-9c78-94f8d2b316c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:54:31.484805Z",
     "iopub.status.busy": "2026-02-02T00:54:31.484231Z",
     "iopub.status.idle": "2026-02-02T00:54:31.512280Z",
     "shell.execute_reply": "2026-02-02T00:54:31.511513Z",
     "shell.execute_reply.started": "2026-02-02T00:54:31.484764Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from typing import List\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "MAX_LENGTH = 640\n",
    "\n",
    "def pad_and_truncate_tokens(token_lists, max_length: int = MAX_LENGTH):\n",
    "    padded = []\n",
    "    for tokens in token_lists:\n",
    "        truncated = tokens[:max_length]\n",
    "        if len(truncated) < max_length:\n",
    "            truncated += [0] * (max_length - len(truncated))\n",
    "        padded.append(truncated)\n",
    "    return padded\n",
    "\n",
    "class UltraBigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = MAX_LENGTH, n_bins: int = 10):\n",
    "        self.max_length = max_length\n",
    "        input_ids = tokenized_samples['input_ids']\n",
    "        self.real_lengths = [min(max_length, len(seq)) for seq in input_ids]\n",
    "        self.samples = pad_and_truncate_tokens(input_ids, max_length)\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = self._create_bins()\n",
    "        for bin_id in self.bins:\n",
    "            random.shuffle(self.bins[bin_id])\n",
    "    \n",
    "    def _create_bins(self):\n",
    "        bins = defaultdict(list)\n",
    "        if not self.real_lengths:\n",
    "            return bins\n",
    "        min_len = min(self.real_lengths)\n",
    "        max_len = max(self.real_lengths)\n",
    "        if min_len == max_len or self.n_bins == 1:\n",
    "            bins[0] = list(range(len(self.samples)))\n",
    "            return bins\n",
    "        bin_size = (max_len - min_len) / self.n_bins\n",
    "        for idx, length in enumerate(self.real_lengths):\n",
    "            bin_id = min(self.n_bins - 1, int((length - min_len) // bin_size))\n",
    "            bins[bin_id].append(idx)\n",
    "        return {k: v for k, v in bins.items() if v}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx], dtype=torch.long), self.real_lengths[idx]\n",
    "\n",
    "class UltraBigBrainBatchSampler(Sampler):\n",
    "    def __init__(self, dataset: UltraBigBrainDataset, batch_size: int, k: int = 10):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        \n",
    "        self.length_to_indices = defaultdict(list)\n",
    "        for idx, length in enumerate(dataset.real_lengths):\n",
    "            self.length_to_indices[length].append(idx)\n",
    "        \n",
    "        self.sorted_lengths = sorted(self.length_to_indices.keys())\n",
    "        for length in self.sorted_lengths:\n",
    "            random.shuffle(self.length_to_indices[length])\n",
    "        \n",
    "        self.batches = self._create_batches()\n",
    "    \n",
    "    def _create_batches(self):\n",
    "        batches = []\n",
    "        length_to_indices = {k: v.copy() for k, v in self.length_to_indices.items()}\n",
    "        sorted_lengths = self.sorted_lengths.copy()\n",
    "        \n",
    "        while sorted_lengths:\n",
    "            batch = []\n",
    "            start_length = sorted_lengths[0]\n",
    "            available_lengths = [\n",
    "                length for length in sorted_lengths \n",
    "                if abs(length - start_length) <= self.k\n",
    "            ]\n",
    "            \n",
    "            while len(batch) < self.batch_size and available_lengths:\n",
    "                current_length = available_lengths[0]\n",
    "                if not length_to_indices[current_length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if current_length in sorted_lengths:\n",
    "                        sorted_lengths.remove(current_length)\n",
    "                    continue\n",
    "                \n",
    "                batch.append(length_to_indices[current_length].pop())\n",
    "                if len(batch) == self.batch_size:\n",
    "                    break\n",
    "                \n",
    "                if not length_to_indices[current_length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if current_length in sorted_lengths:\n",
    "                        sorted_lengths.remove(current_length)\n",
    "            \n",
    "            if batch:\n",
    "                batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = self.batches.copy()\n",
    "        random.shuffle(batches)\n",
    "        yield from batches\n",
    "\n",
    "def ultra_brain_collate_fn(batch_items):\n",
    "    tensors, lengths = zip(*batch_items)\n",
    "    max_len = max(lengths)\n",
    "    trimmed_tensors = [tensor[:max_len] for tensor in tensors]\n",
    "    return torch.stack(trimmed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import bisect\n",
    "\n",
    "class UltraDuperBigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, packing_type, max_length=MAX_LENGTH):\n",
    "        packed_samples = []\n",
    "        packed_seq_ids = []\n",
    "\n",
    "        samples = [s[:max_length] for s in tokenized_samples[\"input_ids\"]]\n",
    "\n",
    "        if packing_type == \"basic\":\n",
    "            packed_samples, packed_seq_ids = self._basic_pack(samples, max_length)\n",
    "        elif packing_type == \"ffd\":\n",
    "            packed_samples, packed_seq_ids = self._ffd_pack(samples, max_length)\n",
    "        elif packing_type == \"obfd\":\n",
    "            packed_samples, packed_seq_ids = self._obfd_pack(samples, max_length)\n",
    "        else:\n",
    "            raise ValueError(packing_type)\n",
    "\n",
    "        self.input_ids = torch.cat(packed_samples, dim=0)\n",
    "        self.seq_ids = torch.cat(packed_seq_ids, dim=0)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def _basic_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "        buf, buf_ids = [], []\n",
    "        sid = 0\n",
    "\n",
    "        for s in samples:\n",
    "            for t in s:\n",
    "                buf.append(t)\n",
    "                buf_ids.append(sid)\n",
    "                if len(buf) == max_length:\n",
    "                    inputs.append(self._make_input(buf, max_length))\n",
    "                    seqs.append(self._make_seq_ids(buf_ids, max_length))\n",
    "                    buf, buf_ids = [], []\n",
    "            sid += 1\n",
    "\n",
    "        if buf:\n",
    "            inputs.append(self._make_input(buf, max_length))\n",
    "            seqs.append(self._make_seq_ids(buf_ids, max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def _ffd_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "\n",
    "        sequences = [(i, s) for i, s in enumerate(samples) if len(s) <= max_length]\n",
    "        sequences.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        bins = []\n",
    "\n",
    "        for sid, seq in sequences:\n",
    "            placed = False\n",
    "            for b in bins:\n",
    "                if b[\"rem\"] >= len(seq):\n",
    "                    b[\"tok\"].extend(seq)\n",
    "                    b[\"ids\"].extend([sid] * len(seq))\n",
    "                    b[\"rem\"] -= len(seq)\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed:\n",
    "                bins.append({\n",
    "                    \"tok\": list(seq),\n",
    "                    \"ids\": [sid] * len(seq),\n",
    "                    \"rem\": max_length - len(seq),\n",
    "                })\n",
    "\n",
    "        for b in bins:\n",
    "            inputs.append(self._make_input(b[\"tok\"], max_length))\n",
    "            seqs.append(self._make_seq_ids(b[\"ids\"], max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def _obfd_pack(self, samples, max_length):\n",
    "        inputs, seqs = [], []\n",
    "\n",
    "        sequences = [(i, s) for i, s in enumerate(samples) if len(s) <= max_length]\n",
    "        sequences.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        bins = []\n",
    "        free = []\n",
    "\n",
    "        for sid, seq in sequences:\n",
    "            l = len(seq)\n",
    "            idx = bisect.bisect_left(free, (l, -1))\n",
    "\n",
    "            if idx < len(free):\n",
    "                _, bidx = free.pop(idx)\n",
    "                b = bins[bidx]\n",
    "                b[\"tok\"].extend(seq)\n",
    "                b[\"ids\"].extend([sid] * l)\n",
    "                b[\"rem\"] -= l\n",
    "                bisect.insort(free, (b[\"rem\"], bidx))\n",
    "            else:\n",
    "                bidx = len(bins)\n",
    "                bins.append({\n",
    "                    \"tok\": list(seq),\n",
    "                    \"ids\": [sid] * l,\n",
    "                    \"rem\": max_length - l,\n",
    "                })\n",
    "                bisect.insort(free, (max_length - l, bidx))\n",
    "\n",
    "        for b in bins:\n",
    "            inputs.append(self._make_input(b[\"tok\"], max_length))\n",
    "            seqs.append(self._make_seq_ids(b[\"ids\"], max_length))\n",
    "\n",
    "        return inputs, seqs\n",
    "\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def _make_input(self, tokens, max_length):\n",
    "        return torch.tensor(\n",
    "            tokens + [0] * (max_length - len(tokens)),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def _make_seq_ids(self, seq_ids, max_length):\n",
    "        return torch.tensor(\n",
    "            seq_ids + [-1] * (max_length - len(seq_ids)),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"seq_ids\": self.seq_ids[idx],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62116bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:19:06.706229Z",
     "iopub.status.busy": "2026-02-02T00:19:06.705689Z",
     "iopub.status.idle": "2026-02-02T00:19:06.723836Z",
     "shell.execute_reply": "2026-02-02T00:19:06.723030Z",
     "shell.execute_reply.started": "2026-02-02T00:19:06.706197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.benchmark import Timer\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "class DataMode(Enum):\n",
    "    BRAIN = 1\n",
    "    BIG_BRAIN = 2\n",
    "    ULTRA_BIG_BRAIN = 3\n",
    "    ULTRA_DUPER_BIG_BRAIN = 4\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "def get_dataloader(data_mode: DataMode, n_bins: int | None = None, k: int | None = None, packing_type: str | None = None):\n",
    "    if data_mode == DataMode.BRAIN:\n",
    "        dataset = BrainDataset(tokenized_samples)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32, pin_memory=True)\n",
    "    if data_mode == DataMode.BIG_BRAIN:\n",
    "        dataset = BigBrainDataset(tokenized_samples)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=32, pin_memory=True)\n",
    "    if data_mode == DataMode.ULTRA_BIG_BRAIN:\n",
    "        if n_bins is None:\n",
    "            n_bins = 5\n",
    "        if k is None:\n",
    "            k = 5\n",
    "        dataset = UltraBigBrainDataset(tokenized_samples, n_bins=n_bins)\n",
    "        sampler = UltraBigBrainBatchSampler(\n",
    "            dataset=dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            k=k,\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_sampler=sampler, \n",
    "            num_workers=32,\n",
    "            pin_memory=True,\n",
    "            collate_fn=ultra_brain_collate_fn\n",
    "        )\n",
    "    if data_mode == DataMode.ULTRA_DUPER_BIG_BRAIN:\n",
    "        dataset = UltraDuperBigBrainDataset(tokenized_samples, packing_type)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7ef6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T21:59:03.809670Z",
     "iopub.status.busy": "2026-02-01T21:59:03.809331Z",
     "iopub.status.idle": "2026-02-01T21:59:03.820644Z",
     "shell.execute_reply": "2026-02-01T21:59:03.819815Z",
     "shell.execute_reply.started": "2026-02-01T21:59:03.809649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "142453e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:19:44.985464Z",
     "iopub.status.busy": "2026-02-02T00:19:44.984155Z",
     "iopub.status.idle": "2026-02-02T00:19:45.049819Z",
     "shell.execute_reply": "2026-02-02T00:19:45.049112Z",
     "shell.execute_reply.started": "2026-02-02T00:19:44.985437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def run_epoch(data_mode: DataMode, model, optimizer, criterion, device, \n",
    "              warmup_batches=3, n_bins: int | None = None, k: int | None = None):\n",
    "    dataloader = get_dataloader(data_mode, n_bins, k)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(device.type) if device.type == 'cuda' else None\n",
    "    batch_times = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_batch_size = 0\n",
    "    total_seq_len = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        is_warmup = batch_idx < warmup_batches\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        if not is_warmup:\n",
    "            start_time = time.perf_counter()\n",
    "        \n",
    "        inputs = batch.to(device)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len = inputs.size(1)\n",
    "        total_batch_size += batch_size\n",
    "        total_seq_len += batch_size * seq_len\n",
    "        \n",
    "        targets = inputs[:, 1:]\n",
    "        inputs = inputs[:, :-1]\n",
    "        src = inputs.transpose(0, 1)\n",
    "        tgt_y = targets.reshape(-1)\n",
    "        mask = generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                out = model(src, mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                logits = out.reshape(-1, out.size(-1))\n",
    "                loss = criterion(logits, tgt_y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        if not is_warmup:\n",
    "            batch_time = time.perf_counter() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if is_warmup:\n",
    "            progress_bar.set_postfix(\n",
    "                status=\"warmup\", \n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len\n",
    "            )\n",
    "        else:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
    "            \n",
    "            avg_batch_size = total_batch_size / (batch_idx + 1)\n",
    "            avg_seq_len = total_seq_len / total_batch_size\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                avg_loss=f\"{avg_loss:.4f}\",\n",
    "                batch_time=f\"{batch_time:.4f}s\",\n",
    "                avg_batch_time=f\"{avg_batch_time:.4f}s\",\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "                avg_batch_size=f\"{avg_batch_size:.1f}\",\n",
    "                avg_seq_len=f\"{avg_seq_len:.1f}\"\n",
    "            )\n",
    "\n",
    "    time_stats = {}\n",
    "    if batch_times:\n",
    "        time_array = np.array(batch_times)\n",
    "        time_stats = {\n",
    "            'batch_time_min': float(np.min(time_array)),\n",
    "            'batch_time_max': float(np.max(time_array)),\n",
    "            'batch_time_mean': float(np.mean(time_array)),\n",
    "            'batch_time_median': float(np.median(time_array)),\n",
    "            'batch_time_std': float(np.std(time_array)),\n",
    "            'num_batches': len(batch_times)\n",
    "        }\n",
    "        \n",
    "        total_time = sum(batch_times)\n",
    "        throughput = total_samples / total_time\n",
    "        avg_time = total_time / len(batch_times)\n",
    "        final_avg_loss = total_loss / total_samples\n",
    "        \n",
    "        final_avg_batch_size = total_batch_size / len(progress_bar)\n",
    "        final_avg_seq_len = total_seq_len / total_batch_size\n",
    "        \n",
    "        print(f\"Avg loss: {final_avg_loss:.4f}\")\n",
    "        print(f\"Avg batch time: {avg_time:.4f}s\")\n",
    "        print(f\"Throughput: {throughput:.2f} samples/s\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Avg batch size: {final_avg_batch_size:.2f}\")\n",
    "        print(f\"Avg sequence length: {final_avg_seq_len:.2f}\")\n",
    "        print(f\"Batch time stats: min={time_stats['batch_time_min']:.4f}s, \"\n",
    "              f\"max={time_stats['batch_time_max']:.4f}s, \"\n",
    "              f\"mean={time_stats['batch_time_mean']:.4f}s, \"\n",
    "              f\"median={time_stats['batch_time_median']:.4f}s\")\n",
    "    \n",
    "    stats = {\n",
    "        'avg_loss': final_avg_loss if batch_times else 0,\n",
    "        'throughput': throughput if batch_times else 0,\n",
    "        'total_samples': total_samples,\n",
    "        'avg_batch_size': final_avg_batch_size if batch_times else 0,\n",
    "        'avg_seq_len': final_avg_seq_len if batch_times else 0,\n",
    "        'time_stats': time_stats,  # Только статистика, а не все времена\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d376b1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:00:31.670496Z",
     "iopub.status.busy": "2026-02-02T01:00:31.670002Z",
     "iopub.status.idle": "2026-02-02T01:15:59.838015Z",
     "shell.execute_reply": "2026-02-02T01:15:59.837008Z",
     "shell.execute_reply.started": "2026-02-02T01:00:31.670465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [15:07<00:00,  5.26it/s, avg_batch_size=64.0, avg_batch_time=0.1861s, avg_loss=2.7787, avg_seq_len=640.0, batch_size=26, batch_time=0.0858s, loss=2.5784, seq_len=640]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 2.7787\n",
      "Avg batch time: 0.1861s\n",
      "Throughput: 344.13 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 640.00\n",
      "Batch time stats: min=0.0858s, max=0.2001s, mean=0.1861s, median=0.1861s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_brain = run_epoch(\n",
    "    DataMode.BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "518d147e-5f37-4ebf-9dbb-2ae712184128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T22:48:32.607729Z",
     "iopub.status.busy": "2026-02-01T22:48:32.607296Z",
     "iopub.status.idle": "2026-02-01T22:48:32.622292Z",
     "shell.execute_reply": "2026-02-01T22:48:32.621502Z",
     "shell.execute_reply.started": "2026-02-01T22:48:32.607704Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"This DataLoader will create\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The current process just got forked\")\n",
    "\n",
    "# Fix tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccdcd62f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:23:11.266473Z",
     "iopub.status.busy": "2026-02-02T01:23:11.265496Z",
     "iopub.status.idle": "2026-02-02T01:38:31.183847Z",
     "shell.execute_reply": "2026-02-02T01:38:31.182880Z",
     "shell.execute_reply.started": "2026-02-02T01:23:11.266446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [15:07<00:00,  5.26it/s, avg_batch_size=64.0, avg_batch_time=0.1860s, avg_loss=2.7496, avg_seq_len=640.0, batch_size=26, batch_time=0.0850s, loss=2.3751, seq_len=640]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 2.7496\n",
      "Avg batch time: 0.1860s\n",
      "Throughput: 344.25 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 640.00\n",
      "Batch time stats: min=0.0850s, max=0.1935s, mean=0.1860s, median=0.1860s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_big_brain = run_epoch(\n",
    "    DataMode.BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4058286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:18:20.594274Z",
     "iopub.status.busy": "2026-02-01T23:18:20.593155Z",
     "iopub.status.idle": "2026-02-01T23:18:20.686082Z",
     "shell.execute_reply": "2026-02-01T23:18:20.685143Z",
     "shell.execute_reply.started": "2026-02-01T23:18:20.594244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16741844463793543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([1 if len(t) > 640 else 0 for t in tokenized_samples['input_ids']]) / len(tokenized_samples['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9056d92-4545-48cb-a679-b7e90d8e2362",
   "metadata": {},
   "source": [
    "For batch 64 at most always there are samples with seq_len > 640 in batch, so we dont see avg_batch_time improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0c661ff-1f62-4b53-9fbe-8825093a69af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:20:21.978428Z",
     "iopub.status.busy": "2026-02-01T23:20:21.977439Z",
     "iopub.status.idle": "2026-02-01T23:20:22.009891Z",
     "shell.execute_reply": "2026-02-01T23:20:22.008974Z",
     "shell.execute_reply.started": "2026-02-01T23:20:21.978386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('brain.json', 'w') as f:\n",
    "    json.dump(batch_times_brain, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dc7603c-8c9c-49c3-995d-7753b8d82857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T23:20:37.937832Z",
     "iopub.status.busy": "2026-02-01T23:20:37.937376Z",
     "iopub.status.idle": "2026-02-01T23:20:37.964674Z",
     "shell.execute_reply": "2026-02-01T23:20:37.963827Z",
     "shell.execute_reply.started": "2026-02-01T23:20:37.937809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('big_brain.json', 'w') as f:\n",
    "    json.dump(batch_times_big_brain, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d9bb5dd-7ac2-4c5e-a76d-2558c9c29524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:19:48.777658Z",
     "iopub.status.busy": "2026-02-02T00:19:48.777156Z",
     "iopub.status.idle": "2026-02-02T00:28:18.504484Z",
     "shell.execute_reply": "2026-02-02T00:28:18.503318Z",
     "shell.execute_reply.started": "2026-02-02T00:19:48.777630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:17<00:00,  9.59it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.2030, avg_seq_len=324.5, batch_size=64, batch_time=0.0767s, loss=4.9446, seq_len=246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.2030\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.29 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0214s, max=0.2078s, mean=0.1011s, median=0.0947s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k10 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d3ed3df-93df-44da-9d04-964e4cd97fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:29:53.599554Z",
     "iopub.status.busy": "2026-02-02T00:29:53.598858Z",
     "iopub.status.idle": "2026-02-02T00:38:20.907023Z",
     "shell.execute_reply": "2026-02-02T00:38:20.905999Z",
     "shell.execute_reply.started": "2026-02-02T00:29:53.599511Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:18<00:00,  9.58it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.2091, avg_seq_len=324.5, batch_size=64, batch_time=0.1216s, loss=5.0844, seq_len=406] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.2091\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.22 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0219s, max=0.2013s, mean=0.1011s, median=0.0945s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k1 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f97f9b7-580d-44d9-baf0-3291b693d2cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T00:38:39.811658Z",
     "iopub.status.busy": "2026-02-02T00:38:39.811148Z",
     "iopub.status.idle": "2026-02-02T00:47:04.030004Z",
     "shell.execute_reply": "2026-02-02T00:47:04.028880Z",
     "shell.execute_reply.started": "2026-02-02T00:38:39.811632Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4774/4774 [08:18<00:00,  9.57it/s, avg_batch_size=64.0, avg_batch_time=0.1011s, avg_loss=5.1857, avg_seq_len=324.5, batch_size=64, batch_time=0.1042s, loss=4.9141, seq_len=347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 5.1857\n",
      "Avg batch time: 0.1011s\n",
      "Throughput: 633.10 samples/s\n",
      "Total samples: 305498\n",
      "Avg batch size: 63.99\n",
      "Avg sequence length: 324.53\n",
      "Batch time stats: min=0.0219s, max=0.2019s, mean=0.1011s, median=0.0948s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_big_brain_k50 = run_epoch(\n",
    "    DataMode.ULTRA_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    n_bins=20,\n",
    "    k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4d9b5fe-ba09-4fd0-a82a-515cd4fc9440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:42:55.644356Z",
     "iopub.status.busy": "2026-02-02T01:42:55.643840Z",
     "iopub.status.idle": "2026-02-02T01:42:55.665339Z",
     "shell.execute_reply": "2026-02-02T01:42:55.664049Z",
     "shell.execute_reply.started": "2026-02-02T01:42:55.644331Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expand_time_stats(row):\n",
    "    \"\"\"Раскрывает time_stats в отдельные колонки.\"\"\"\n",
    "    if 'time_stats' in row:\n",
    "        time_stats = row.pop('time_stats')\n",
    "        row.update({\n",
    "            'batch_time_min': time_stats.get('batch_time_min'),\n",
    "            'batch_time_max': time_stats.get('batch_time_max'),\n",
    "            'batch_time_mean': time_stats.get('batch_time_mean'),\n",
    "            'batch_time_median': time_stats.get('batch_time_median'),\n",
    "        })\n",
    "    return row\n",
    "\n",
    "def create_benchmark_df(results_list, config_names=None):\n",
    "    \"\"\"Создает DataFrame из списка результатов.\"\"\"\n",
    "    expanded_results = [expand_time_stats(r.copy()) for r in results_list]\n",
    "    df = pd.DataFrame(expanded_results)\n",
    "    \n",
    "    if config_names and len(config_names) == len(results_list):\n",
    "        df.insert(0, 'config', config_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def append_benchmark_result(df, result, config_name=None):\n",
    "    \"\"\"Добавляет результат в DataFrame.\"\"\"\n",
    "    expanded_result = expand_time_stats(result.copy())\n",
    "    \n",
    "    if config_name:\n",
    "        expanded_result['config'] = config_name\n",
    "    \n",
    "    new_row = pd.DataFrame([expanded_result])\n",
    "    return pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Использование\n",
    "df = create_benchmark_df(\n",
    "    [batch_times_ultra_big_brain_k1],\n",
    "    config_names=['ultrabigbrain k=1']\n",
    ")\n",
    "\n",
    "# Добавить еще результат\n",
    "df = append_benchmark_result(df, batch_times_ultra_big_brain_k10, 'ultrabigbrain k=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48de3080-4cce-44b0-86b3-75dc5394e3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:43:01.023935Z",
     "iopub.status.busy": "2026-02-02T01:43:01.023483Z",
     "iopub.status.idle": "2026-02-02T01:43:01.049491Z",
     "shell.execute_reply": "2026-02-02T01:43:01.048766Z",
     "shell.execute_reply.started": "2026-02-02T01:43:01.023913Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>avg_loss</th>\n",
       "      <th>throughput</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>avg_batch_size</th>\n",
       "      <th>avg_seq_len</th>\n",
       "      <th>batch_time_min</th>\n",
       "      <th>batch_time_max</th>\n",
       "      <th>batch_time_mean</th>\n",
       "      <th>batch_time_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultrabigbrain k=1</td>\n",
       "      <td>5.209138</td>\n",
       "      <td>633.223054</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.201310</td>\n",
       "      <td>0.101121</td>\n",
       "      <td>0.094524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultrabigbrain k=10</td>\n",
       "      <td>5.203040</td>\n",
       "      <td>633.287550</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.207838</td>\n",
       "      <td>0.101111</td>\n",
       "      <td>0.094678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               config  avg_loss  ...  batch_time_mean  batch_time_median\n",
       "0   ultrabigbrain k=1  5.209138  ...         0.101121           0.094524\n",
       "1  ultrabigbrain k=10  5.203040  ...         0.101111           0.094678\n",
       "\n",
       "[2 rows x 10 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec6462ed-8b7a-497f-bac3-5206d9299f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:43:04.515498Z",
     "iopub.status.busy": "2026-02-02T01:43:04.515125Z",
     "iopub.status.idle": "2026-02-02T01:43:04.540198Z",
     "shell.execute_reply": "2026-02-02T01:43:04.539418Z",
     "shell.execute_reply.started": "2026-02-02T01:43:04.515476Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>avg_loss</th>\n",
       "      <th>throughput</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>avg_batch_size</th>\n",
       "      <th>avg_seq_len</th>\n",
       "      <th>batch_time_min</th>\n",
       "      <th>batch_time_max</th>\n",
       "      <th>batch_time_mean</th>\n",
       "      <th>batch_time_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultrabigbrain k=1</td>\n",
       "      <td>5.209138</td>\n",
       "      <td>633.223054</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.201310</td>\n",
       "      <td>0.101121</td>\n",
       "      <td>0.094524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultrabigbrain k=10</td>\n",
       "      <td>5.203040</td>\n",
       "      <td>633.287550</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.207838</td>\n",
       "      <td>0.101111</td>\n",
       "      <td>0.094678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultrabigbrain k=50</td>\n",
       "      <td>5.185742</td>\n",
       "      <td>633.097297</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.201899</td>\n",
       "      <td>0.101141</td>\n",
       "      <td>0.094805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               config  avg_loss  ...  batch_time_mean  batch_time_median\n",
       "0   ultrabigbrain k=1  5.209138  ...         0.101121           0.094524\n",
       "1  ultrabigbrain k=10  5.203040  ...         0.101111           0.094678\n",
       "2  ultrabigbrain k=50  5.185742  ...         0.101141           0.094805\n",
       "\n",
       "[3 rows x 10 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = append_benchmark_result(df, batch_times_ultra_big_brain_k50, 'ultrabigbrain k=50')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8aa81e1d-bdd2-4adb-958e-8d7dd07ebdca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:43:19.600543Z",
     "iopub.status.busy": "2026-02-02T01:43:19.600086Z",
     "iopub.status.idle": "2026-02-02T01:43:19.616432Z",
     "shell.execute_reply": "2026-02-02T01:43:19.615678Z",
     "shell.execute_reply.started": "2026-02-02T01:43:19.600520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = append_benchmark_result(df, batch_times_big_brain, 'big brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1eb39a0-a8b1-421b-9439-e662fd6e739e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:44:16.843358Z",
     "iopub.status.busy": "2026-02-02T01:44:16.842262Z",
     "iopub.status.idle": "2026-02-02T01:44:16.871150Z",
     "shell.execute_reply": "2026-02-02T01:44:16.870338Z",
     "shell.execute_reply.started": "2026-02-02T01:44:16.843329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json('report.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7bb6daa7-df82-463b-afaf-c267d1f2da35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:43:32.346839Z",
     "iopub.status.busy": "2026-02-02T01:43:32.346400Z",
     "iopub.status.idle": "2026-02-02T01:43:32.362178Z",
     "shell.execute_reply": "2026-02-02T01:43:32.361353Z",
     "shell.execute_reply.started": "2026-02-02T01:43:32.346816Z"
    }
   },
   "outputs": [],
   "source": [
    "df = append_benchmark_result(df, batch_times_brain, 'brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e384f347-67c4-4902-901b-9b1e4074f9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:44:59.764630Z",
     "iopub.status.busy": "2026-02-02T01:44:59.764084Z",
     "iopub.status.idle": "2026-02-02T01:44:59.791308Z",
     "shell.execute_reply": "2026-02-02T01:44:59.790580Z",
     "shell.execute_reply.started": "2026-02-02T01:44:59.764593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>avg_loss</th>\n",
       "      <th>throughput</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>avg_batch_size</th>\n",
       "      <th>avg_seq_len</th>\n",
       "      <th>batch_time_min</th>\n",
       "      <th>batch_time_max</th>\n",
       "      <th>batch_time_mean</th>\n",
       "      <th>batch_time_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultrabigbrain k=1</td>\n",
       "      <td>5.209138</td>\n",
       "      <td>633.223054</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.201310</td>\n",
       "      <td>0.101121</td>\n",
       "      <td>0.094524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultrabigbrain k=10</td>\n",
       "      <td>5.203040</td>\n",
       "      <td>633.287550</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.207838</td>\n",
       "      <td>0.101111</td>\n",
       "      <td>0.094678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultrabigbrain k=50</td>\n",
       "      <td>5.185742</td>\n",
       "      <td>633.097297</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>324.526956</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.201899</td>\n",
       "      <td>0.101141</td>\n",
       "      <td>0.094805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big brain</td>\n",
       "      <td>2.749565</td>\n",
       "      <td>344.246500</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.085038</td>\n",
       "      <td>0.193472</td>\n",
       "      <td>0.186007</td>\n",
       "      <td>0.186009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brain</td>\n",
       "      <td>2.778737</td>\n",
       "      <td>344.130712</td>\n",
       "      <td>305498</td>\n",
       "      <td>63.99204</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.085790</td>\n",
       "      <td>0.200119</td>\n",
       "      <td>0.186070</td>\n",
       "      <td>0.186064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               config  avg_loss  ...  batch_time_mean  batch_time_median\n",
       "0   ultrabigbrain k=1  5.209138  ...         0.101121           0.094524\n",
       "1  ultrabigbrain k=10  5.203040  ...         0.101111           0.094678\n",
       "2  ultrabigbrain k=50  5.185742  ...         0.101141           0.094805\n",
       "3           big brain  2.749565  ...         0.186007           0.186009\n",
       "4               brain  2.778737  ...         0.186070           0.186064\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb937d-b25a-46fe-b94a-0cc8c5d5c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_ultra_duper_big_brain(\n",
    "    data_mode: DataMode,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    warmup_batches=3,\n",
    "    n_bins: int | None = None,\n",
    "    k: int | None = None,\n",
    "    packing_type: str | None = None,\n",
    "):\n",
    "    dataloader = get_dataloader(data_mode, n_bins, k, packing_type)\n",
    "    model.train().to(device)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device.type) if device.type == \"cuda\" else None\n",
    "\n",
    "    batch_times = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_batch_size = 0\n",
    "    total_seq_len = 0\n",
    "\n",
    "    nhead = model.transformer_encoder.layers[0].self_attn.num_heads\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        is_warmup = batch_idx < warmup_batches\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        if not is_warmup:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "        # ---------------- data ----------------\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        seq_ids   = batch[\"seq_ids\"].to(device)\n",
    "\n",
    "        inputs  = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "        seq_ids = seq_ids[:, :-1]\n",
    "\n",
    "        B, L = inputs.shape\n",
    "\n",
    "        # ---------------- attention mask ----------------\n",
    "        causal = torch.tril(\n",
    "            torch.ones(L, L, device=device, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        same_seq = seq_ids.unsqueeze(1) == seq_ids.unsqueeze(2)\n",
    "\n",
    "        allowed = causal & same_seq\n",
    "        allowed.diagonal(dim1=-2, dim2=-1).fill_(True)\n",
    "\n",
    "        attn_mask = ~allowed  # 🔥 IMPORTANT: True = MASKED\n",
    "\n",
    "        attn_mask = (\n",
    "            attn_mask.unsqueeze(1)\n",
    "            .expand(-1, nhead, -1, -1)\n",
    "            .reshape(B * nhead, L, L)\n",
    "        )\n",
    "\n",
    "\n",
    "        # ---------------- stats ----------------\n",
    "        total_batch_size += B\n",
    "        total_seq_len += B * L\n",
    "\n",
    "        src = inputs.transpose(0, 1)\n",
    "        tgt_y = targets.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ---------------- forward / backward ----------------\n",
    "        if scaler:\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                out = model(src, attn_mask)\n",
    "                logits = out.transpose(0, 1).reshape(-1, out.size(-1))\n",
    "                loss = criterion(logits, tgt_y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(src, attn_mask)\n",
    "            logits = out.transpose(0, 1).reshape(-1, out.size(-1))\n",
    "            loss = criterion(logits, tgt_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # ---------------- timing ----------------\n",
    "        if not is_warmup:\n",
    "            batch_time = time.perf_counter() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * B\n",
    "        total_samples += B\n",
    "\n",
    "        # ---------------- logging ----------------\n",
    "        if is_warmup:\n",
    "            progress_bar.set_postfix(\n",
    "                status=\"warmup\",\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                batch_size=B,\n",
    "                seq_len=L,\n",
    "            )\n",
    "        else:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "            avg_batch_size = total_batch_size / (batch_idx + 1)\n",
    "            avg_seq_len = total_seq_len / total_batch_size\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                avg_loss=f\"{avg_loss:.4f}\",\n",
    "                batch_time=f\"{batch_time:.4f}s\",\n",
    "                avg_batch_time=f\"{avg_batch_time:.4f}s\",\n",
    "                batch_size=B,\n",
    "                seq_len=L,\n",
    "                avg_batch_size=f\"{avg_batch_size:.1f}\",\n",
    "                avg_seq_len=f\"{avg_seq_len:.1f}\",\n",
    "            )\n",
    "\n",
    "    # ---------------- summary ----------------\n",
    "    time_stats = {}\n",
    "    if batch_times:\n",
    "        time_array = np.array(batch_times)\n",
    "        time_stats = {\n",
    "            'batch_time_min': float(np.min(time_array)),\n",
    "            'batch_time_max': float(np.max(time_array)),\n",
    "            'batch_time_mean': float(np.mean(time_array)),\n",
    "            'batch_time_median': float(np.median(time_array)),\n",
    "            'batch_time_std': float(np.std(time_array)),\n",
    "            'num_batches': len(batch_times)\n",
    "        }\n",
    "        \n",
    "        total_time = sum(batch_times)\n",
    "        throughput = total_samples / total_time\n",
    "        avg_time = total_time / len(batch_times)\n",
    "        final_avg_loss = total_loss / total_samples\n",
    "        \n",
    "        final_avg_batch_size = total_batch_size / len(progress_bar)\n",
    "        final_avg_seq_len = total_seq_len / total_batch_size\n",
    "        \n",
    "        print(f\"Avg loss: {final_avg_loss:.4f}\")\n",
    "        print(f\"Avg batch time: {avg_time:.4f}s\")\n",
    "        print(f\"Throughput: {throughput:.2f} samples/s\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Avg batch size: {final_avg_batch_size:.2f}\")\n",
    "        print(f\"Avg sequence length: {final_avg_seq_len:.2f}\")\n",
    "        print(f\"Batch time stats: min={time_stats['batch_time_min']:.4f}s, \"\n",
    "              f\"max={time_stats['batch_time_max']:.4f}s, \"\n",
    "              f\"mean={time_stats['batch_time_mean']:.4f}s, \"\n",
    "              f\"median={time_stats['batch_time_median']:.4f}s\")\n",
    "    \n",
    "    stats = {\n",
    "        'avg_loss': final_avg_loss if batch_times else 0,\n",
    "        'throughput': throughput if batch_times else 0,\n",
    "        'total_samples': total_samples,\n",
    "        'avg_batch_size': final_avg_batch_size if batch_times else 0,\n",
    "        'avg_seq_len': final_avg_seq_len if batch_times else 0,\n",
    "        'time_stats': time_stats,  # Только статистика, а не все времена\n",
    "    }\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64775346",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_times_ultra_duper_big_brain_basic = run_epoch_ultra_duper_big_brain(\n",
    "    DataMode.ULTRA_DUPER_BIG_BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device,\n",
    "    packing_type='basic'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

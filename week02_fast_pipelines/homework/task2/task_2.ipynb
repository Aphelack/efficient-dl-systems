{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fec75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korenikil/efficient-dl-systems/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import Sampler, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6553c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ad7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModel\n",
    "\n",
    "def get_gpt2_model(device = 'cpu') -> torch.nn.Module:\n",
    "    return TransformerModel(\n",
    "        ntoken=tokenizer.vocab_size,\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        d_hid=1024,\n",
    "        nlayers=8,\n",
    "        dropout=0.1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904d3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wikitext_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read().split('\\n = ')\n",
    "\n",
    "def get_train_samples(data_path):\n",
    "    articles_1 =  process_wikitext_file(f\"{data_path}/train-00000-of-00002.txt\")\n",
    "    articles_2 =  process_wikitext_file(f\"{data_path}/train-00001-of-00002.txt\")\n",
    "    articles = articles_1 + articles_2\n",
    "    return articles\n",
    "\n",
    "data_path = 'wikitext-103-raw-v1'\n",
    "articles = get_train_samples(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68fbf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenizer(\n",
    "    articles,\n",
    "    return_tensors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb884542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305498"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_samples['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d37242e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16741844463793543"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([1 if len(t) > 640 else 0 for t in tokenized_samples['input_ids']]) / len(tokenized_samples['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0205c214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1027, 11748,  ...,     0,     0,     0],\n",
       "        [  101,  1027, 11247,  ...,  2000,  2367,  3131]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 640\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_and_truncate_tokens(\n",
    "    tokens,\n",
    "    max_length: int = MAX_LENGTH,\n",
    "    pad_token_id: int = 0,\n",
    "    effective: bool = False\n",
    "):\n",
    "    max_len_in_batch = max(len(seq) for seq in tokens)\n",
    "    effective_max = min(max_len_in_batch, max_length) if effective else max_length\n",
    "    \n",
    "    padded_sequences = []\n",
    "    for seq in tokens:\n",
    "        if len(seq) > effective_max:\n",
    "            seq = seq[:effective_max]\n",
    "        if len(seq) < max_length:\n",
    "            seq = seq + [pad_token_id] * (max_length - len(seq))\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "pad_and_truncate_tokens(tokenized_samples['input_ids'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 640\n",
    "\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples: str, max_length: int = MAX_LENGTH):\n",
    "        self.samples = pad_and_truncate_tokens(tokenized_samples['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        assert idx >= 0 and idx < len(self.samples)\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88598a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = MAX_LENGTH):\n",
    "        self.samples = tokenized_samples['input_ids']\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        assert idx >= 0 and idx < len(self.samples)\n",
    "        return self.samples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def collate_fn(\n",
    "    batch, max_length: int = MAX_LENGTH\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pad each sequence of the incoming sequences list\n",
    "    :param batch: a list of the objects received from the dataset by __getitem__\n",
    "    :param max_length: maximum sequence length to pad to (for \"Brain\" approach only)\n",
    "    :return: tuple of padded sequences and corresponding training targets\n",
    "    \"\"\"\n",
    "    return pad_and_truncate_tokens(batch, max_length, effective=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from typing import List, Dict\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class UltraBigBrainDataset(Dataset):\n",
    "    def __init__(self, tokenized_samples, max_length: int = 512, n_bins: int = 10):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        input_ids = tokenized_samples['input_ids']\n",
    "        self.samples = self._pad_tokens(input_ids, max_length)\n",
    "        \n",
    "        self.lengths = [min(max_length, len(seq)) for seq in input_ids]\n",
    "        self.n_bins = n_bins\n",
    "        \n",
    "        self.bins = defaultdict(list)\n",
    "        min_len = min(self.lengths)\n",
    "        max_len = max(self.lengths)\n",
    "        \n",
    "        if min_len == max_len:\n",
    "            self.bins[0] = list(range(len(self.samples)))\n",
    "        else:\n",
    "            bin_size = (max_len - min_len) / n_bins\n",
    "            for idx, length in enumerate(self.lengths):\n",
    "                bin_id = min(n_bins - 1, int((length - min_len) // bin_size))\n",
    "                self.bins[bin_id].append(idx)\n",
    "        \n",
    "        for bin_id in self.bins:\n",
    "            random.shuffle(self.bins[bin_id])\n",
    "    \n",
    "    def _pad_tokens(self, token_lists, max_length):\n",
    "        padded = []\n",
    "        for tokens in token_lists:\n",
    "            truncated = tokens[:max_length]\n",
    "            if len(truncated) < max_length:\n",
    "                truncated += [0] * (max_length - len(truncated))\n",
    "            padded.append(truncated)\n",
    "        return padded\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx], dtype=torch.long)\n",
    "\n",
    "class UltraBigBrainBatchSampler(Sampler):\n",
    "    def __init__(self, dataset: UltraBigBrainDataset, batch_size: int, k: int = 10):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        \n",
    "        self.length_to_indices = defaultdict(list)\n",
    "        for idx, length in enumerate(dataset.lengths):\n",
    "            self.length_to_indices[length].append(idx)\n",
    "        \n",
    "        self.sorted_lengths = sorted(self.length_to_indices.keys())\n",
    "        for length in self.sorted_lengths:\n",
    "            random.shuffle(self.length_to_indices[length])\n",
    "        \n",
    "        self.batches = self._create_batches()\n",
    "    \n",
    "    def _create_batches(self):\n",
    "        batches = []\n",
    "        length_to_indices = {k: v.copy() for k, v in self.length_to_indices.items()}\n",
    "        sorted_lengths = self.sorted_lengths.copy()\n",
    "        \n",
    "        while sorted_lengths:\n",
    "            batch = []\n",
    "            start_length = sorted_lengths[0]\n",
    "            \n",
    "            available_lengths = [\n",
    "                length for length in sorted_lengths \n",
    "                if abs(length - start_length) <= self.k\n",
    "            ]\n",
    "            \n",
    "            while len(batch) < self.batch_size and available_lengths:\n",
    "                length = available_lengths[0]\n",
    "                \n",
    "                if not length_to_indices[length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if length in sorted_lengths:\n",
    "                        sorted_lengths.remove(length)\n",
    "                    continue\n",
    "                \n",
    "                batch.append(length_to_indices[length].pop())\n",
    "                \n",
    "                if len(batch) == self.batch_size:\n",
    "                    break\n",
    "                \n",
    "                if not length_to_indices[length]:\n",
    "                    available_lengths.pop(0)\n",
    "                    if length in sorted_lengths:\n",
    "                        sorted_lengths.remove(length)\n",
    "            \n",
    "            if batch:\n",
    "                batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = self.batches.copy()\n",
    "        random.shuffle(batches)\n",
    "        yield from batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9510a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraDuperBigBrainDataset(Dataset):\n",
    "    def __init__(self, data_path: str, max_length: int = MAX_LENGTH):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62116bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.benchmark import Timer\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "class DataMode(Enum):\n",
    "    BRAIN = 1\n",
    "    BIG_BRAIN = 2\n",
    "    ULTRA_BIG_BRAIN = 3\n",
    "    ULTRA_DUPER_BIG_BRAIN = 4\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def get_dataloader(data_mode: DataMode):\n",
    "    if data_mode == DataMode.BRAIN:\n",
    "        dataset = BrainDataset(data_path)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32, pin_memory=True)\n",
    "    if data_mode == DataMode.BIG_BRAIN:\n",
    "        dataset = BigBrainDataset(data_path)\n",
    "        return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142453e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_mode: DataMode, model, optimizer, criterion, device='cpu'):\n",
    "    dataloader = get_dataloader(data_mode)\n",
    "    batch_times = []\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    scaler = GradScaler(device.type)\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        tgt = inputs[:, 1:]\n",
    "        inp = inputs[:, :-1]\n",
    "\n",
    "        src = inp.transpose(0, 1)\n",
    "        tgt_y = tgt.reshape(-1)\n",
    "        mask = generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "\n",
    "        def training_step():\n",
    "            with autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                out = model(src, mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                logits = out.reshape(-1, out.size(-1))\n",
    "                loss = criterion(logits, tgt_y)\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            return loss\n",
    "\n",
    "        if batch_idx >= 3:\n",
    "            timer = Timer(\n",
    "                stmt=\"training_step()\",\n",
    "                globals={\"training_step\": training_step},\n",
    "                num_threads=torch.get_num_threads(),\n",
    "            )\n",
    "            m = timer.timeit(1)\n",
    "            bt = m.mean\n",
    "        else:\n",
    "            bt = 0\n",
    "\n",
    "        # call training_step exactly once\n",
    "        loss = training_step()\n",
    "        batch_loss = loss.item()\n",
    "\n",
    "        bs = inputs.size(0)\n",
    "        total_loss += batch_loss * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        if batch_idx >= 3 and batch_idx > 0:\n",
    "            batch_times.append(bt)\n",
    "            avg_bt = sum(batch_times) / len(batch_times)\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{batch_loss:.4f}\",\n",
    "                avg=f\"{(total_loss/total_samples):.4f}\",\n",
    "                t=f\"{bt:.3f}\",\n",
    "                avg_t=f\"{avg_bt:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            progress_bar.set_postfix(status=\"warmup\")\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "\n",
    "    if batch_times:\n",
    "        avg_bt = sum(batch_times) / len(batch_times)\n",
    "        print(f\"epoch loss: {epoch_loss:.4f}\")\n",
    "        print(f\"batch time avg {avg_bt:.3f}s min {min(batch_times):.3f}s max {max(batch_times):.3f}s\")\n",
    "        print(f\"throughput: {total_samples / sum(batch_times):.1f} samples/s\")\n",
    "\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korenikil/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(data_mode, model, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     34\u001b[0m     timer \u001b[38;5;241m=\u001b[39m Timer(\n\u001b[1;32m     35\u001b[0m         stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step()\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_step},\n\u001b[1;32m     37\u001b[0m         num_threads\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mget_num_threads(),\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     bt \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:274\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mExecute the main statement (`stmt`) `number` times.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m common\u001b[38;5;241m.\u001b[39mset_torch_threads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_spec\u001b[38;5;241m.\u001b[39mnum_threads):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m common\u001b[38;5;241m.\u001b[39mMeasurement(\n\u001b[1;32m    277\u001b[0m         number_per_run\u001b[38;5;241m=\u001b[39mnumber,\n\u001b[1;32m    278\u001b[0m         raw_times\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeit(number\u001b[38;5;241m=\u001b[39mnumber)],\n\u001b[1;32m    279\u001b[0m         task_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_spec\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:264\u001b[0m, in \u001b[0;36mTimer._timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_timeit\u001b[39m(\u001b[38;5;28mself\u001b[39m, number: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# take less than 1 ns. (And this prevents divide by zero errors.)\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1e-9\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mrun_epoch.<locals>.training_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/efficient-dl-systems/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_gpt2_model(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss = run_epoch(\n",
    "    DataMode.BRAIN,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcd62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e693ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4058286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c055427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korenikil/efficient-dl-systems/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "\n",
    "from unet import Unet\n",
    "\n",
    "from dataset import get_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75268e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossScaler:\n",
    "    def __init__(self, init_scale):\n",
    "        self.scale = init_scale\n",
    "\n",
    "    def get_scale(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        pass\n",
    "\n",
    "    def _scale_gradients(self, optimizer):\n",
    "        has_nan_or_inf = False\n",
    "        has_zero = False\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            for param in param_group['params']:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.div_(self.scale)\n",
    "\n",
    "                    grad = param.grad.data\n",
    "                    if torch.isinf(grad).any() or torch.isnan(grad).any():\n",
    "                        has_nan_or_inf = True\n",
    "                        param.grad.data.zero_()\n",
    "\n",
    "                    if param.grad.data.abs().sum().item() == 0:\n",
    "                        has_zero = True\n",
    "\n",
    "        if has_zero:\n",
    "            print(f'Zero gradients with scale {self.scale}')\n",
    "        return has_nan_or_inf\n",
    "\n",
    "\n",
    "class StaticLossScaler(LossScaler):\n",
    "    def __init__(self, init_scale):\n",
    "        super().__init__(init_scale)\n",
    "\n",
    "    def get_scale(self):\n",
    "        return self.scale\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        has_nan_of_inf = self._scale_gradients(optimizer)\n",
    "\n",
    "        if not has_nan_of_inf:\n",
    "            optimizer.step()\n",
    "\n",
    "class DynamicLossScaler(LossScaler):\n",
    "    def __init__(self, init_scale):\n",
    "        super().__init__(init_scale)\n",
    "\n",
    "    def get_scale(self):\n",
    "        return self.scale\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        has_nan_or_inf = self._scale_gradients(optimizer)\n",
    "\n",
    "        if not has_nan_or_inf:\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            self._update_scale()\n",
    "\n",
    "    def _update_scale(self):\n",
    "        self.scale /= 2\n",
    "        print(f'Scale updated: {self.scale}')\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    train_loader: DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    scaler: LossScaler\n",
    ") -> None:\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with autocast(device.type, dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        # TODO: your code for loss scaling here\n",
    "        optimizer.zero_grad()\n",
    "        scale = scaler.get_scale()\n",
    "\n",
    "        scaled_loss = scale * loss\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        accuracy = ((outputs > 0.5) == labels).float().mean()\n",
    "\n",
    "        pbar.set_description(f\"Loss: {round(loss.item(), 4)} \" f\"Accuracy: {round(accuracy.item() * 100, 4)}\")\n",
    "\n",
    "def train(loss_scaling, device_id):\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    model = Unet().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_loader = get_train_data()\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_epoch(train_loader, model, criterion, optimizer, device, loss_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "683cf824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6044 Accuracy: 95.163: 100%|██████████| 40/40 [00:32<00:00,  1.21it/s] \n",
      "Loss: 0.5942 Accuracy: 97.6608: 100%|██████████| 40/40 [00:32<00:00,  1.22it/s]\n",
      "Loss: 0.5869 Accuracy: 98.5456: 100%|██████████| 40/40 [00:32<00:00,  1.21it/s]\n",
      "Loss: 0.5833 Accuracy: 98.546: 100%|██████████| 40/40 [00:32<00:00,  1.22it/s] \n",
      "Loss: 0.5851 Accuracy: 98.8529: 100%|██████████| 40/40 [00:33<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "init_scale = 2.0**16\n",
    "static_scaler = StaticLossScaler(init_scale)\n",
    "\n",
    "train(static_scaler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91227f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8495 Accuracy: 61.0644:   2%|▎         | 1/40 [00:03<02:22,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero gradients with scale 1048576.0\n",
      "Scale updated: 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6092 Accuracy: 94.6808: 100%|██████████| 40/40 [00:33<00:00,  1.20it/s]\n",
      "Loss: 0.5933 Accuracy: 97.2532: 100%|██████████| 40/40 [00:32<00:00,  1.22it/s]\n",
      "Loss: 0.5875 Accuracy: 98.516: 100%|██████████| 40/40 [00:33<00:00,  1.21it/s] \n",
      "Loss: 0.5845 Accuracy: 98.6235: 100%|██████████| 40/40 [00:33<00:00,  1.19it/s]\n",
      "Loss: 0.5804 Accuracy: 98.6768: 100%|██████████| 40/40 [00:33<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "init_scale = 2.0**20\n",
    "dynamic_scaler = DynamicLossScaler(init_scale)\n",
    "\n",
    "train(dynamic_scaler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a6e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
